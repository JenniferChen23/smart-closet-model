{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0469d7",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29f6fd",
   "metadata": {},
   "source": [
    "Image Similarity using CNN feature embeddings\n",
    "https://github.com/totogot/ImageSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87eaf1",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382f786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "class Img2Vec:\n",
    "    def __init__(self, model_name='resnet50', weights='DEFAULT'):\n",
    "        self.architecture = model_name\n",
    "        self.weights = weights\n",
    "        self.transform = self.assign_transform(weights)\n",
    "        self.device = self.set_device()\n",
    "        self.model = self.initiate_model()\n",
    "        self.embed = self.assign_layer()\n",
    "        print(\"Model initialized\")\n",
    "\n",
    "    def assign_transform(self, weights):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def set_device(self):\n",
    "        return 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    def initiate_model(self):\n",
    "        model = getattr(models, self.architecture)(weights=self.weights)\n",
    "        model.to(self.device)\n",
    "        return model.eval()\n",
    "\n",
    "    def assign_layer(self):\n",
    "        return nn.Sequential(*list(self.model.children())[:-1])\n",
    "\n",
    "    def embed_image(self, img_path):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_trans = self.transform(img).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            embedding = self.embed(img_trans)\n",
    "        return embedding.squeeze()\n",
    "    \n",
    "\n",
    "    def embed_images(self, upper_img_path, lower_img_path):\n",
    "        upper_embedding = self.embed_image(upper_img_path)\n",
    "        lower_embedding = self.embed_image(lower_img_path)\n",
    "        #print(f\"Upper image tensor shape: {upper_embedding}\")\n",
    "        #print(f\"Lower image tensor shape: {lower_image_tensor.shape}\")\n",
    "        # Concatenate the two embeddings\n",
    "        combined_embedding = torch.cat((upper_embedding, lower_embedding), dim=0)\n",
    "        \n",
    "        return upper_embedding, lower_embedding, combined_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf11c99",
   "metadata": {},
   "source": [
    "# 產生 combine embeding 為4096的向量，因為太大存進 csv 會變亂碼，所以改用 pickle 存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292d734",
   "metadata": {},
   "source": [
    "### 好的Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a027378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n",
      "Combined embedding shape: (4096,)\n",
      "ok\n",
      "  index                                 img_pathE  \\\n",
      "0   0_1  ../new_data/cut_style:america/0_1_E_.jpg   \n",
      "1   0_3  ../new_data/cut_style:america/0_3_E_.jpg   \n",
      "2   0_4  ../new_data/cut_style:america/0_4_E_.jpg   \n",
      "3   0_7  ../new_data/cut_style:america/0_7_E_.jpg   \n",
      "4   0_8  ../new_data/cut_style:america/0_8_E_.jpg   \n",
      "\n",
      "                                  img_pathQ  \\\n",
      "0  ../new_data/cut_style:america/0_1_Q_.jpg   \n",
      "1  ../new_data/cut_style:america/0_3_Q_.jpg   \n",
      "2  ../new_data/cut_style:america/0_4_Q_.jpg   \n",
      "3  ../new_data/cut_style:america/0_7_Q_.jpg   \n",
      "4  ../new_data/cut_style:america/0_8_Q_.jpg   \n",
      "\n",
      "                                              predEQ label  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0011892455, 0.0, 0.0, 2...     1   \n",
      "1  [0.0, 0.0, 0.0071198223, 0.14539345, 0.0, 0.0,...     1   \n",
      "2  [0.0, 0.0, 0.0, 0.20410387, 0.17157467, 0.1296...     1   \n",
      "3  [0.0, 0.005689638, 0.0, 0.12158056, 0.13749401...     1   \n",
      "4  [0.0, 0.0, 0.27391717, 0.0, 0.62258655, 0.0019...     1   \n",
      "\n",
      "                                              embedE  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0011892455, 0.0, 0.0, 2...   \n",
      "1  [0.0, 0.0, 0.0071198223, 0.14539345, 0.0, 0.0,...   \n",
      "2  [0.0, 0.0, 0.0, 0.20410387, 0.17157467, 0.1296...   \n",
      "3  [0.0, 0.005689638, 0.0, 0.12158056, 0.13749401...   \n",
      "4  [0.0, 0.0, 0.27391717, 0.0, 0.62258655, 0.0019...   \n",
      "\n",
      "                                              embedQ  \n",
      "0  [0.066727616, 0.0, 0.06931917, 0.12752776, 0.0...  \n",
      "1  [0.123964064, 0.0, 0.014558438, 0.25938794, 0....  \n",
      "2  [0.06421464, 0.0, 0.1558264, 0.07901376, 0.077...  \n",
      "3  [0.018466124, 0.0, 0.82570106, 0.1992205, 0.00...  \n",
      "4  [0.20184408, 0.0, 0.017478613, 0.0, 0.07155952...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "# 创建一个空的 DataFrame\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    'index': [],\n",
    "    'img_pathE': [],\n",
    "    'img_pathQ': [],\n",
    "    'predEQ': [],\n",
    "    'label': [],\n",
    "    'embedE': [],\n",
    "    'embedQ': []\n",
    "}\n",
    "\n",
    "\n",
    "img2vec = Img2Vec('resnet50', weights='IMAGENET1K_V2')\n",
    "\n",
    "\n",
    "df=pd.read_csv(r'C:\\Users\\chen\\資管專題\\good_outfits.csv')\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    upper_picture_path = df.at[index, 'img_pathE'].replace(':', '_').replace('../new_data/cut_style', 'C://Users//chen//資管專題//data//cut_style')\n",
    "    lower_picture_path = df.at[index, 'img_pathQ'].replace(':', '_').replace('../new_data/cut_style', 'C://Users//chen//資管專題//data//cut_style')\n",
    "    \n",
    "    upper_embedding, lower_embedding, combined_embedding = img2vec.embed_images(upper_picture_path, lower_picture_path)\n",
    "    \n",
    "    upper_embedding = np.array(upper_embedding)\n",
    "    lower_embedding = np.array(lower_embedding)\n",
    "    combined_embedding = np.array(combined_embedding)\n",
    "    \n",
    "    data['index'].append(df.at[index, 'index'])\n",
    "    data['img_pathE'].append(df.at[index, 'img_pathE'])\n",
    "    data['img_pathQ'].append(df.at[index, 'img_pathQ'])\n",
    "    data['label'].append('1')\n",
    "    data['predEQ'].append(combined_embedding)#('['+','.join(map(str, combined_embedding))+']')\n",
    "    data['embedE'].append(upper_embedding)#('['+','.join(map(str, upper_embedding))+']')\n",
    "    data['embedQ'].append(lower_embedding)#('['+','.join(map(str, lower_embedding))+']')\n",
    " \n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data.to_pickle('data.pkl')\n",
    "print(f\"Combined embedding shape: {combined_embedding.shape}\")\n",
    "print('ok')\n",
    "# df['img_pathE'] = df['img_pathE'].str.replace(':', '_').str.replace('../new_data/cut_style', 'C://Users//chen//資管專題//data//cut_style')\n",
    "# df['img_pathQ'] = df['img_pathQ'].str.replace(':', '_').str.replace('../new_data/cut_style', 'C://Users//chen//資管專題//data//cut_style')\n",
    "print(data.head())\n",
    "\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2860d8cd",
   "metadata": {},
   "source": [
    "### 糟的Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca56c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "# 创建一个空的 DataFrame\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    'idxE': [],\n",
    "    'idxQ': [],\n",
    "    'img_pathE': [],\n",
    "    'img_pathQ': [],\n",
    "    'predEQ': [],\n",
    "    'label': [],\n",
    "    'embedE': [],\n",
    "    'embedQ': []\n",
    "}\n",
    "\n",
    "\n",
    "img2vec = Img2Vec('resnet50', weights='IMAGENET1K_V2')\n",
    "\n",
    "\n",
    "df=pd.read_csv(r'C:\\Users\\chen\\資管專題\\bad_outfits.csv')\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    upper_picture_path = df.at[index, 'img_pathE'].replace(':', '_').replace('../new_data/cut_style', 'C://Users//chen//資管專題//data//cut_style')\n",
    "    lower_picture_path = df.at[index, 'img_pathQ'].replace(':', '_').replace('../new_data/cut_style', 'C://Users//chen//資管專題//data//cut_style')\n",
    "    \n",
    "    upper_embedding, lower_embedding, combined_embedding = img2vec.embed_images(upper_picture_path, lower_picture_path)\n",
    "    \n",
    "    upper_embedding = np.array(upper_embedding)\n",
    "    lower_embedding = np.array(lower_embedding)\n",
    "    combined_embedding = np.array(combined_embedding)\n",
    "    \n",
    "    data['idxE'].append(df.at[index, 'idxE'])\n",
    "    data['idxQ'].append(df.at[index, 'idxQ'])\n",
    "    data['img_pathE'].append(df.at[index, 'img_pathE'])\n",
    "    data['img_pathQ'].append(df.at[index, 'img_pathQ'])\n",
    "    data['label'].append('0')\n",
    "    data['predEQ'].append(combined_embedding)#('['+','.join(map(str, combined_embedding))+']')\n",
    "    data['embedE'].append(upper_embedding)#('['+','.join(map(str, upper_embedding))+']')\n",
    "    data['embedQ'].append(lower_embedding)#('['+','.join(map(str, lower_embedding))+']')\n",
    " \n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data.to_pickle('data.pkl')\n",
    "print(f\"Combined embedding shape: {combined_embedding.shape}\")\n",
    "print('ok')\n",
    "# df['img_pathE'] = df['img_pathE'].str.replace(':', '_').str.replace('../new_data/cut_style', 'C://Users//chen//資管專題//data//cut_style')\n",
    "# df['img_pathQ'] = df['img_pathQ'].str.replace(':', '_').str.replace('../new_data/cut_style', 'C://Users//chen//資管專題//data//cut_style')\n",
    "print(data.head())\n",
    "\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a63c1655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m     combined_embedding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     upper_embedding, lower_embedding, combined_embedding \u001b[38;5;241m=\u001b[39m img2vec\u001b[38;5;241m.\u001b[39membed_images(upper_picture_path, lower_picture_path)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Convert to numpy arrays if not None\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m upper_embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[5], line 44\u001b[0m, in \u001b[0;36mImg2Vec.embed_images\u001b[1;34m(self, upper_img_path, lower_img_path)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_images\u001b[39m(\u001b[38;5;28mself\u001b[39m, upper_img_path, lower_img_path):\n\u001b[1;32m---> 44\u001b[0m     upper_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_image(upper_img_path)\n\u001b[0;32m     45\u001b[0m     lower_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_image(lower_img_path)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m#print(f\"Upper image tensor shape: {upper_embedding}\")\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m#print(f\"Lower image tensor shape: {lower_image_tensor.shape}\")\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Concatenate the two embeddings\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 39\u001b[0m, in \u001b[0;36mImg2Vec.embed_image\u001b[1;34m(self, img_path)\u001b[0m\n\u001b[0;32m     37\u001b[0m img_trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 39\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(img_trans)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\models\\resnet.py:146\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    144\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m--> 146\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m    147\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m    148\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "# 创建一个空的 DataFrame\n",
    "data = {\n",
    "    'index': [],\n",
    "    'img_path': [],\n",
    "    'predEQ': [],\n",
    "    'label': [],\n",
    "    'embedE': [],\n",
    "    'embedQ': []\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "img2vec = Img2Vec('resnet50', weights='IMAGENET1K_V2')\n",
    "\n",
    "three_style = ['korea','japan','america']\n",
    "three_style_index={'korea':'2','japan':'1','america':'0'}\n",
    "three_style_each_num =1059\n",
    "ten_occation=['wedding_guest','travel','sports','shopping','school','porm','party','dating','daily_work','conference']\n",
    "ten_occation_index={'wedding_guest':'12','travel':'5','sports':'6','shopping':'9','school':'11','porm':'8','party':'10','dating':'3','daily_work':'4','conference':'7'}\n",
    "ten_occation_each_num = 1005\n",
    "\n",
    "\n",
    "for style in three_style:\n",
    "    file_path='C://Users//chen//資管專題//data//'+'cut_style_'+ style +'//'\n",
    "    for i in range(1,three_style_each_num+1):\n",
    "        img_path = '../new_data/style:'+style+'/'+ str(i)+'.jpg'\n",
    "        upper_picture = three_style_index[style]+'_'+str(i)+'_E_.jpg'\n",
    "        lower_picture = three_style_index[style]+'_'+str(i)+'_Q_.jpg'\n",
    "    #upper_embedding, lower_embedding, combined_embedding = img2vec.embed_images(r\"C:\\Users\\chen\\資管專題\\skirt.png\", r\"C:\\Users\\chen\\資管專題\\skirt.png\")\n",
    "\n",
    "        upper_picture_path = os.path.join(file_path, upper_picture)\n",
    "        lower_picture_path = os.path.join(file_path, lower_picture)\n",
    "\n",
    "        if os.path.exists(lower_picture_path)==False and os.path.exists(upper_picture_path)==False:\n",
    "            continue\n",
    "        elif os.path.exists(lower_picture_path)==False:\n",
    "            upper_embedding = img2vec.embed_image(upper_picture_path)\n",
    "            lower_embedding=None\n",
    "            combined_embedding=None\n",
    "        elif os.path.exists(upper_picture_path)==False:\n",
    "            lower_embedding = img2vec.embed_image(lower_picture_path)\n",
    "            upper_embeddin=None\n",
    "            combined_embedding=None\n",
    "        else:\n",
    "            upper_embedding, lower_embedding, combined_embedding = img2vec.embed_images(upper_picture_path, lower_picture_path)\n",
    "        \n",
    "        # Convert to numpy arrays if not None\n",
    "        if upper_embedding is not None:\n",
    "            upper_embedding = np.array(upper_embedding)\n",
    "        if lower_embedding is not None:\n",
    "            lower_embedding = np.array(lower_embedding)\n",
    "        if combined_embedding is not None:\n",
    "            combined_embedding = np.array(combined_embedding)\n",
    "            \n",
    "        #print(f\"Upper embedding: {upper_embedding}\")\n",
    "        #print(three_style_index[style]+'_'+str(i))\n",
    "        data['index'].append(three_style_index[style] + '_' + str(i))\n",
    "        data['img_path'].append(img_path)\n",
    "        data['predEQ'].append(combined_embedding)\n",
    "        data['embedE'].append(upper_embedding)\n",
    "        data['embedQ'].append(lower_embedding)\n",
    "        data['label'].append(1)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    name = 'cut_style_'+ style+'.csv'\n",
    "    df.to_csv(name, index=False)\n",
    "    print('ok')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1aefd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Upper embedding shape: {upper_embedding.shape}\")\n",
    "print(f\"Upper embedding: {upper_embedding}\")\n",
    "print(f\"Lower embedding shape: {lower_embedding.shape}\")\n",
    "print(f\"Lower embedding shape: {lower_embedding}\")\n",
    "print(f\"Combined embedding shape: {combined_embedding.shape}\")\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dating：3\n",
    "Daily Work：4\n",
    "Travel：5\n",
    "Sports：6\n",
    "Conference：7\n",
    "Prom：8\n",
    "Shopping：9\n",
    "Party：10\n",
    "School：11\n",
    "Wedding Guest：12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423baa8",
   "metadata": {},
   "source": [
    "# OpenAI clip api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14552a",
   "metadata": {},
   "source": [
    "https://medium.com/@highsunday0630/image-embedding-1-clip%E6%A8%A1%E5%9E%8B%E6%8F%90%E5%8F%96-image-embedding-%E4%B8%A6%E4%BB%A5-tensorboard-%E8%A6%96%E8%A6%BA%E5%8C%96%E6%95%88%E6%9E%9C-dc281370d7d8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
